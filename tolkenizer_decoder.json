{"class_name": "Tokenizer", "config": {"num_words": null, "filters": "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n", "lower": true, "split": " ", "char_level": true, "oov_token": null, "document_count": 87213, "word_counts": "{\"c\": 3266523, \"e\": 1569120, \"h\": 2110796}", "word_docs": "{\"c\": 87213, \"e\": 62789, \"h\": 72797}", "index_docs": "{\"1\": 87213, \"3\": 62789, \"2\": 72797}", "index_word": "{\"1\": \"c\", \"2\": \"h\", \"3\": \"e\"}", "word_index": "{\"c\": 1, \"h\": 2, \"e\": 3}"}}